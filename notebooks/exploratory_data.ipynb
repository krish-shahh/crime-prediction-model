{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importa for data visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotCrimeCounts(data, features, city):\n",
    "    '''\n",
    "    Data is the dataframe containing the crime statistics.\n",
    "    Features is  disctionary mapping {description: column}\n",
    "    city is the name of the city for which we are plotting.\n",
    "    '''\n",
    "    for description, columnName in features.iteritems():\n",
    "        tmp = data.sort_values(by=columnName)\n",
    "        sns.countplot(y=columnName, data=data)\n",
    "        plt.title('Crimes in {} by {}'.format(city, description))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions to help clean SF data. We do this only once, as we \n",
    "# now save the results using pickle!\n",
    "def bucket(series, n):\n",
    "    # Takes a series and returns a series mapping each element to a\n",
    "    # one of n buckets.\n",
    "    mi, ma = series.min(), series.max()\n",
    "    buckets = np.linspace(mi, ma, n + 1)\n",
    "    \n",
    "    res = np.zeros(len(series))\n",
    "    array = series.values\n",
    "    if np.isnan(array).any():\n",
    "        print \"Error! A total of {} nans!\".format(np.isnan(array).sum())\n",
    "    for i in xrange(n):\n",
    "        res[(buckets[i] <= array) & (array < buckets[i+1])] = i\n",
    "    return res\n",
    "\n",
    "def cleanColumns(data):\n",
    "    # Used to rename the columns in our data grame to their appropriate names.\n",
    "    # Also drops unnecessary columns.\n",
    "    data['Latitude'] = data['Y']\n",
    "    data['Longitude'] = data['X']\n",
    "    data['Type'] = data['Category']\n",
    "    \n",
    "    # print data.columns\n",
    "    data = data.drop(['IncidntNum', 'Descript','Resolution','Address','X','Y', 'Location'], axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def createPartitions(data, n):\n",
    "    # Remove outliers from the latitude/longitude issues\n",
    "    # We know that the city lies between -130, -120 longitude\n",
    "    # We also know that the citiy lies between 37 and 40 degrees latitude\n",
    "    data = data[-120 > data.Longitude][data.Longitude > (-130)]\n",
    "    data = data[data.Latitude > 37][data.Latitude < 40]\n",
    "    \n",
    "    # Each row is an occurrance of a single crime. \n",
    "    # Keep around the original data\n",
    "    # data['Region'] =  n *  buckets(data['Latitude'], n) + buckets(data['Longitude'],n) + 1\n",
    "    data['xRegion'] = bucket(data['Latitude'], n)\n",
    "    data['yRegion'] =  bucket(data['Longitude'],n)\n",
    "    data['Region'] =  n * data.xRegion + data.yRegion    \n",
    "    \n",
    "    # Add in the types into the results.\n",
    "    mapping = {key: i for i,key in enumerate(data['Type'].unique())}\n",
    "    data['TypeIndex'] = data['Type'].map(mapping)\n",
    "\n",
    "    # Now we can add the crime counts. \n",
    "    data['CrimeCount'] = np.ones(len(data))\n",
    "    return data\n",
    "\n",
    "def extractDateFeatures(data):\n",
    "    # Creates a new data frame and returns it as copy with all the data that we're interested in\n",
    "    # Create map from week days to integers\n",
    "    DayOfWeek = {'Sunday': 1,\n",
    "                 'Monday': 2,\n",
    "                 'Tuesday': 3,\n",
    "                 'Wednesday': 4,\n",
    "                 'Thursday': 5,\n",
    "                 'Friday': 6,\n",
    "                 'Saturday': 7 }\n",
    "    data['DoW'] = data['DayOfWeek'].map(DayOfWeek)\n",
    "    data = data.drop('DayOfWeek', axis=1)\n",
    "    print \"Created Weeks\"\n",
    "    \n",
    "    # We assume that the Date column is already in datetime format\n",
    "    data['Month'] = data.Date.map(lambda x: x.month)\n",
    "    data['DoM'] = data.Date.map(lambda x: x.day)\n",
    "    data['Year'] = data.Date.map(lambda x: x.year)\n",
    "    data['ToD'] = data.Time.map(lambda x: x.minute)\n",
    "    data['Time'] = data.Time.map(lambda x: x.value / 10**9) - data.Date.min().value / 10**9\n",
    "    \n",
    "    # We add an additional column that combines the month and the year into number of months since beginning\n",
    "    min_year = data.Year.min()\n",
    "    data['TimeFeature'] = data.ix[:, ['Year', 'Month']].apply(lambda s: 12*(s[0]-min_year) + s[1], axis=1)\n",
    "    \n",
    "    data = data.drop('Date', axis=1)\n",
    "    \n",
    "    print \"Created the time data features!\"\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make histograms of the crimes by month for each given year\n",
    "def histograms(data, city, directory):\n",
    "    years = data.Year.unique()\n",
    "    plt.clf()\n",
    "    for year in years:\n",
    "        # Subset the data\n",
    "        tmp = data[data.Year == year]\n",
    "        \n",
    "        # Make Histogram\n",
    "        sns.countplot(x = 'Month', data=tmp)\n",
    "        plt.title('{} Crime Histogram for {}'.format(city, year))\n",
    "        plt.savefig('../figures/{}/crimes_monthly_{}'.format(\n",
    "                directory, year))\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's do one really large heatmap\n",
    "def createHeatmapData(data, size = 30):\n",
    "    # data is what we're making a heatmap on.\n",
    "    # Using the San Francisco data set, we create an nxn matrix\n",
    "    # which counts the number of crimes in a given area!\n",
    "    res = np.zeros((size, size))\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            res[i,j] = len(data[(data.xRegion == i) & (data.yRegion == j)])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's redo the heatmap year by year?\n",
    "def yearByYear(data, folder, city):\n",
    "    # Find unique years!\n",
    "    years = data.Year.unique()\n",
    "    \n",
    "    plt.clf()  # Clear figure before plotting.\n",
    "    for year in years:\n",
    "        # Deal with data in SF set being zero indexed\n",
    "        tmp = data[data.Year == year]\n",
    "        # Only plot if there's data:\n",
    "        if len(tmp) > 0:\n",
    "            sns.heatmap(createHeatmapData(tmp))\n",
    "            plt.title('Crime Distribution in {} for {}'.format(city, year))\n",
    "            plt.savefig('../figures/{}/heat_map_{}'.format(folder, year))\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's redo it month by month because that's what we want to analyze?\n",
    "def monthByMonth(data, folder, city):\n",
    "    years = data.Year.unique()\n",
    "    months = data.Month.unique()\n",
    "    \n",
    "    plt.clf()  # Clear figure before plotting\n",
    "    for year in years:\n",
    "        for month in months:\n",
    "            tmp = data[(data.Year == year) & (data.Month == month)]\n",
    "            # Only plot if there is data to be plotted\n",
    "            if len(tmp) > 0:\n",
    "                sns.heatmap(createHeatmapData(tmp))\n",
    "                plt.title('Crime Distribution in {} for {}, {}'.format(\n",
    "                        city, month, year))\n",
    "                plt.savefig('../figures/{}/heat_map_{}_{}'.format(\n",
    "                        folder, month, year))\n",
    "                plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we write a function to partition the data\n",
    "def createSimplePartitions(data, n):\n",
    "    # Returns a partitioned version of the bosclean data!\n",
    "    data['xRegion'] = bucket(data.Latitude, n)\n",
    "    data['yRegion'] = bucket(data.Longitude, n)\n",
    "    data['Region'] = n * data.xRegion + data.yRegion\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sfdata_file = '../data/sf.csv'\n",
    "sfdata = pd.read_csv(sfdata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create histograms on some important metrics\n",
    "# Map from english_description : column name\n",
    "columnsToPlotSF = { 'Police District' : 'PdDistrict',\n",
    "                    'Crime Type' : 'Category',\n",
    "                    'Day of the Week' : 'DayOfWeek',\n",
    "                   'Crime Outcome' : 'Resolution' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotCrimeCounts(sfdata, columnsToPlotSF, 'San Francisco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Warnings raised:', [<warnings.WarningMessage object at 0x7fdc17196650>])\n",
      "('Warning message:', 'Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.')\n",
      "(\"Applying <type 'str'> dtype to columns:\", [10])\n"
     ]
    }
   ],
   "source": [
    "# Read bosdata\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "bos_file = '../data/boston.csv'\n",
    "target_type = str  # The desired output type\n",
    "\n",
    "with warnings.catch_warnings(record=True) as ws:\n",
    "    warnings.simplefilter(\"always\")\n",
    "\n",
    "    bosData = pd.read_csv(bos_file, sep=\",\", header=0)\n",
    "    print(\"Warnings raised:\", ws)\n",
    "    # We have an error on specific columns, try and load them as string\n",
    "    for w in ws:\n",
    "        s = str(w.message)\n",
    "        print(\"Warning message:\", s)\n",
    "        match = re.search(r\"Columns \\(([0-9,]+)\\) have mixed types\\.\", s)\n",
    "        if match:\n",
    "            columns = match.group(1).split(',') # Get columns as a list\n",
    "            columns = [int(c) for c in columns]\n",
    "            print(\"Applying %s dtype to columns:\" % target_type, columns)\n",
    "            bosData.iloc[:,columns] = bosData.iloc[:,columns].astype(target_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create histograms on some important metrics for Boston\n",
    "columnsToPlotBos = { 'Reporting District' : 'REPTDISTRICT', \n",
    "                     'Weapon Type' : 'WEAPONTYPE',\n",
    "                     'Shooting/No Shooting' : 'Shooting',\n",
    "                     'Officer Shift' : 'SHIFT',\n",
    "                     'Year' : 'Year', \n",
    "                     'Month' : 'Month',\n",
    "                     'Day of the Week' : 'DAY_WEEK'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotCrimeCounts(bosData, columnsToPlotBos, 'Boston')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loadPickle = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert date to actual date format. This might take a while!\n",
    "# Note that we should not have to do this once the data\n",
    "# has been cleaned!\n",
    "# SKIP IF CLEAN DATA EXISTS\n",
    "if not loadPickle:\n",
    "    sfdata.Date = sfdata['Date'].apply(lambda x: pd.to_datetime(x, errors='raise'))\n",
    "    sfdata.Time = sfdata['Time'].apply(lambda x: pd.to_datetime(x, errors='raise'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sfpickle_file = '../../cs281_data/large_data/sfclean.pk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We only need to do this once, afterwards we should load from the \n",
    "# saved location.\n",
    "# DO NOT RUN IF PICKLED FILE ALREADY EXISTS\n",
    "if not loadPickle:\n",
    "    sfclean = cleanColumns(sfdata)\n",
    "    sfclean = extractDateFeatures(sfclean)\n",
    "    sfclean.to_pickle(sfpickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load sfclean from pickled location!\n",
    "sfclean = open(sfpickle_file)\n",
    "sfclean = pickle.load(sfclean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort the data by time\n",
    "sfclean = sfclean.sort_values(by='TimeFeature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save memory by deleting old data\n",
    "del(sfdata)\n",
    "# del(sfclean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some additional histograms\n",
    "columnsToPlotSF2 = { 'Month' : 'Month',\n",
    "                    'Day of Month' : 'DoM',\n",
    "                    'Year' : 'Year',\n",
    "                   'Hour of Day' : 'ToD' }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotCrimeCounts(sfclean, columnsToPlotSF2, 'San Franscisco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "histograms(sfclean, 'San Francisco', 'sf_data_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's generate some heatmaps for both of these data crimes\n",
    "# Can we overlay these on top of the geographical location???\n",
    "n = 30\n",
    "SFByRegion = createPartitions(sfclean, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/matplotlib/collections.py:590: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if self._edgecolors == str('face'):\n"
     ]
    }
   ],
   "source": [
    "sns.heatmap(createHeatmapData(SFByRegion))\n",
    "plt.title('Crime Distribution in San Francisco')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yearByYear(SFByRegion, 'sf_data_analysis', 'San Francisco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "monthByMonth(SFByRegion, 'sf_data_analysis', 'San Francisco')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We now process the Boston Data: Again, we pickle the results.\n",
    "loadBosPickle = True\n",
    "bos_pickle_file = '../../cs281_data/large_data/bosclean.pk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's process the boston data\n",
    "if not loadBosPickle:\n",
    "    # Clean the columns\n",
    "    bosData['Latitude'] = bosData['X']\n",
    "    bosData['Longitude'] = bosData['Y']\n",
    "    \n",
    "    # Drop unused columns\n",
    "    toDrop = ['X', 'Y']\n",
    "    bosData = bosData.drop(toDrop, axis=1)\n",
    "    \n",
    "    # Extract date features\n",
    "    # day of week\n",
    "    day = np.array(bosData.DAY_WEEK)\n",
    "    day[ day == \"Sunday\"] = 0\n",
    "    day[ day == \"Monday\"] = 1\n",
    "    day[ day == \"Tuesday\"] = 2\n",
    "    day[ day == \"Wednesday\"] = 3\n",
    "    day[ day == \"Thursday\"] = 4\n",
    "    day[ day == \"Friday\"] = 5\n",
    "    day[ day == \"Saturday\"] = 6\n",
    "\n",
    "    date_time = np.array([x.split() for x in bosData.FROMDATE])\n",
    "    date = date_time[:,0]\n",
    "    time = date_time[:,1]\n",
    "    tod = date_time[:,2]\n",
    "\n",
    "    # month, day, year\n",
    "    date = np.array([x.split('/') for x in date])\n",
    "    month = [int(x) for x in date[:,0]]\n",
    "    dom = [int(x) for x in date[:,1]]\n",
    "    year = [int(x) for x in date[:,2]]\n",
    "    min_year = np.min(year)\n",
    "    min_month = np.min(month)\n",
    "    time_feat = np.subtract(year, min_year)*12 + np.subtract(month,min_month)\n",
    "\n",
    "    # time of day\n",
    "    time_c = [x.split(':') for x in time]\n",
    "    time = [int(x[1]) if (y == 'AM' and int(x[0]) == 12) else 60*int(x[0])+int(x[1]) \n",
    "            if (y =='AM' and int(x[0]) != 12) or (int(x[0]) == 12 and y == 'PM') else 12*60+60*int(x[0])+int(x[1]) \n",
    "            for x,y in zip(time_c, tod)]\n",
    "    \n",
    "    # Add them back to the data frame\n",
    "    bosData['Day'] = day\n",
    "    del day\n",
    "    bosData['Month'] = month\n",
    "    del month\n",
    "    bosData['Dom'] = dom\n",
    "    del dom\n",
    "    bosData['Year'] = year\n",
    "    del year\n",
    "    bosData['Time'] = time\n",
    "    del time\n",
    "    bosData['TimeFeature'] = time_feat\n",
    "    del time_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Drop some more unnecessary columns\n",
    "if not loadBosPickle:\n",
    "    toDrop = ['COMPNOS', 'NatureCode', 'Location', 'XSTREETNAME', 'STREETNAME']\n",
    "    toDrop += ['INCIDENT_TYPE_DESCRIPTION', 'MAIN_CRIMECODE', 'REPTDISTRICT']\n",
    "    toDrop += ['REPORTINGAREA', 'FROMDATE', 'WEAPONTYPE', 'Shooting', 'DOMESTIC']\n",
    "    toDrop += ['Location']\n",
    "\n",
    "    bosData = bosData.drop(toDrop, axis=1)\n",
    "    print \"Records before: {}\".format(len(bosData))\n",
    "    bosData = bosData.dropna(axis=0)\n",
    "    print \"Records after: {}\".format(len(bosData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets's save the data\n",
    "if not loadBosPickle:\n",
    "    bosData.to_pickle(bos_pickle_file)\n",
    "    del(bosData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's load the data\n",
    "with open(bos_pickle_file) as bosclean_file:\n",
    "    bosclean = pickle.load(bosclean_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "histograms(bosclean, 'Boston', 'boston_data_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's generate some heatmaps on the boston crime data\n",
    "# Can we overlay these over a boston map?\n",
    "n = 30\n",
    "regionedData = createSimplePartitions(bosclean, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's do a single large heatmap\n",
    "sns.heatmap(createHeatmapData(regionedData, n))\n",
    "plt.title('Crime Distribution in Boston')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yearByYear(regionedData, 'boston_data_analysis', 'Boston')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "monthByMonth(regionedData, 'boston_data_analysis', 'Boston')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Warnings raised:', [])\n",
      "485710\n",
      "('Warnings raised:', [])\n",
      "972433\n",
      "('Warnings raised:', [])\n",
      "1448320\n",
      "('Warnings raised:', [])\n",
      "1917629\n",
      "('Warnings raised:', [])\n",
      "2371247\n",
      "('Warnings raised:', [])\n",
      "2819223\n",
      "('Warnings raised:', [])\n",
      "3256088\n",
      "('Warnings raised:', [])\n",
      "3682969\n",
      "('Warnings raised:', [])\n",
      "4074628\n",
      "('Warnings raised:', [])\n",
      "4444648\n",
      "('Warnings raised:', [])\n",
      "4795983\n",
      "('Warnings raised:', [])\n",
      "5131475\n",
      "('Warnings raised:', [])\n",
      "5437860\n",
      "('Warnings raised:', [])\n",
      "5711613\n",
      "('Warnings raised:', [])\n",
      "5939364\n"
     ]
    }
   ],
   "source": [
    "# Read in the Chicago data set\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "years = 2000 + np.linspace(1,15, 15)\n",
    "buckets = np.zeros(1)\n",
    "target_type = str  # The desired output type\n",
    "\n",
    "chicagoData = pd.DataFrame()\n",
    "for year in years:\n",
    "\n",
    "    data_file = '../../cs281_data/large_data/chicago/%d.0.csv' % year    \n",
    "    with warnings.catch_warnings(record=True) as ws:\n",
    "        warnings.simplefilter(\"always\")\n",
    "\n",
    "        data = pd.read_csv(data_file, sep=\",\", header=None)\n",
    "        print(\"Warnings raised:\", ws)\n",
    "        # We have an error on specific columns, try and load them as string\n",
    "        for w in ws:\n",
    "            s = str(w.message)\n",
    "            print(\"Warning message:\", s)\n",
    "            match = re.search(r\"Columns \\(([0-9,]+)\\) have mixed types\\.\", s)\n",
    "            if match:\n",
    "                columns = match.group(1).split(',') # Get columns as a list\n",
    "                columns = [int(c) for c in columns]\n",
    "                print(\"Applying %s dtype to columns:\" % target_type, columns)\n",
    "                data.iloc[:,columns] = data.iloc[:,columns].astype(target_type)\n",
    "        \n",
    "    chicagoData = chicagoData.append(data,ignore_index=True)\n",
    "    print len(chicagoData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract Features\n",
    "chicagoData.columns = ['Date', 'Latitude', 'Longitude']\n",
    "pickledChicago = True\n",
    "chicago_pickle_file = '../../cs281_data/large_data/chicago.pkl'\n",
    "if not pickledChicago:\n",
    "    \n",
    "    date_time = np.array([x.split() for x in chicagoData.Date])\n",
    "    date = date_time[:,0]\n",
    "    time = date_time[:,1]\n",
    "    tod = date_time[:,2]\n",
    "\n",
    "    # month, day, year\n",
    "    date = np.array([x.split('/') for x in date])\n",
    "    month = [int(x) for x in date[:,0]]\n",
    "    dom = [int(x) for x in date[:,1]]\n",
    "    year = [int(x) for x in date[:,2]]\n",
    "    min_year = np.min(year)\n",
    "    min_month = np.min(month)\n",
    "    time_feat = np.subtract(year, min_year)*12 + np.subtract(month,min_month)\n",
    "    \n",
    "    time_c = [x.split(':') for x in time]\n",
    "    time = [int(x[1]) if (y == 'AM' and int(x[0]) == 12) else 60*int(x[0])+int(x[1]) \n",
    "            if (y =='AM' and int(x[0]) != 12) or (int(x[0]) == 12 and y == 'PM') else 12*60+60*int(x[0])+int(x[1]) \n",
    "            for x,y in zip(time_c, tod)]\n",
    "    \n",
    "    chicagoData['Month'] = month\n",
    "    del month\n",
    "    chicagoData['DoM'] = dom\n",
    "    del dom\n",
    "    chicagoData['Year'] = year\n",
    "    del year\n",
    "    chicagoData['ToD'] = tod\n",
    "    del tod\n",
    "    chicagoData['Time'] = time\n",
    "    del time\n",
    "    chicagoData['TimeFeature'] = time_feat\n",
    "    del time_feat\n",
    "    \n",
    "    # Sort the results\n",
    "    chicagoData = chicagoData.sort_values(by='TimeFeature')\n",
    "    \n",
    "    chicagoData = chicagoData.dropna()\n",
    "    \n",
    "    # Save the results to a pickle file\n",
    "    chicagoData.to_pickle(chicago_pickle_file)\n",
    "    print \"Saved to Pickle File\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "# Let's load the data \n",
    "with open(chicago_pickle_file) as f:\n",
    "    %time\n",
    "    chicagoClean = pickle.load(f)\n",
    "del chicagoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create heatmaps and histograms if possible\n",
    "columnsToPlotChicago = { 'Year' : 'Year', \n",
    "                         'Month' : 'Month',\n",
    "                         'Day of Month' : 'DoM',\n",
    "                         'Hour of Day' : 'ToD'}\n",
    "plotCrimeCounts(chicagoClean, columnsToPlotChicago, 'Chicago')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make histograms of the crime\n",
    "histograms(chicagoClean, 'Chicago', 'chicago_data_analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "# Partition into n = 30 \n",
    "n = 30\n",
    "%time\n",
    "chicagoPartitions = createSimplePartitions(chicagoClean, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 15 µs\n"
     ]
    }
   ],
   "source": [
    "# Heatmap of the entire Chicago region!\n",
    "%time\n",
    "sns.heatmap(createHeatmapData(chicagoPartitions, n))\n",
    "plt.title('Crime Distribution in Chicago')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "yearByYear(chicagoPartitions, 'chicago_data_analysis', 'Chicago')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10 µs\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "monthByMonth(chicagoPartitions, 'chicago_data_analysis', 'Chicago')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "THE BELOW HAS BEEN MOVED TO BASELINES.ipython\n",
    "'''\n",
    "# Calculate averages for each region based on the training set\n",
    "# Last year\n",
    "last_year = sfclean.Year.max()\n",
    "training = sfclean[sfclean.Year != last_year]\n",
    "test = sfclean[sfclean.Year == last_year]\n",
    "\n",
    "# Partition the data\n",
    "averageTrainingData = createPartitions(training, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "years = averageTrainingData.Year.unique()\n",
    "months = averageTrainingData.Month.unique()\n",
    "regions = averageTrainingData.Region.unique()\n",
    "\n",
    "# Calculate how many months of data we have\n",
    "nmonths = 0\n",
    "for year in years:\n",
    "    nmonths += len(averageTrainingData[averageTrainingData.Year == year].Month.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmonths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now calculat ethe predictions!\n",
    "predictions = {}\n",
    "for region in regions:\n",
    "    data = partitionedData[partitionedData.Region == region]\n",
    "    crimes = len(data)\n",
    "    predictions[region] = crimes / float(nmonths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now calculate the rmse\n",
    "testAverage = createPartitions(test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's create a dictionary with the counts for region,month,year\n",
    "from collections import defaultdict\n",
    "crimeCounts = defaultdict(int)\n",
    "for i, row in testAverage.iterrows():\n",
    "    crimeCounts[(row.Region, row.Year, row.Month)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now calculate the RMSE of this function:\n",
    "error = []\n",
    "for region in testAverage.Region.unique():\n",
    "    data = testAverage[testAverage.Region == region]\n",
    "    for year in data.Year.unique():\n",
    "        data2 = data[data.Year == year]\n",
    "        for month in data2.Month.unique():\n",
    "            try:\n",
    "                # print crimeCounts[(region, year, month)]\n",
    "                # print predictions[region]\n",
    "                error.append(crimeCounts[(region, year, month)] - \n",
    "                         predictions[region])\n",
    "            except KeyError:\n",
    "                print region\n",
    "\n",
    "error = np.array(error)\n",
    "rmse = np.sqrt(np.sum(error ** 2) / len(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.std(crimeCounts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.max(crimeCounts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data from sf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sfdata = pd.read_csv(\"../data/sf.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sfdata.PdDistrict.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MISSION', 'PARK', 'INGLESIDE', 'SOUTHERN', 'TENDERLOIN',\n",
       "       'NORTHERN', 'TARAVAL', 'RICHMOND', 'CENTRAL', 'BAYVIEW'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sfdata.PdDistrict.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = sfdata\n",
    "test['CrimeCount'] = np.ones(len(sfdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = sfdata.groupby('PdDistrict')\n",
    "aggregate = test.aggregate(np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5ef8808550>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.countplot(x=\"PdDistrict\", data=sfdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-bd0286b5678b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Convert date to actual date format. This might take a while!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msfdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msfdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msfdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msfdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/core/series.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   2047\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimestamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2049\u001b[1;33m         \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2050\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2051\u001b[0m             \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/src/inference.pyx\u001b[0m in \u001b[0;36mpandas.lib.map_infer (pandas/lib.c:56990)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-bd0286b5678b>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Convert date to actual date format. This might take a while!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msfdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msfdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0msfdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msfdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Time'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/tseries/tools.pyc\u001b[0m in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, utc, box, format, exact, coerce, unit, infer_datetime_format)\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_convert_listlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_convert_listlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m \u001b[0marg\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mDateParseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/pandas/tseries/tools.pyc\u001b[0m in \u001b[0;36m_convert_listlike\u001b[1;34m(arg, box, format)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 result = tslib.array_to_datetime(arg, raise_=errors == 'raise',\n\u001b[0;32m    318\u001b[0m                                                  \u001b[0mutc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mutc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdayfirst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdayfirst\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                                                  coerce=coerce, unit=unit)\n\u001b[0m\u001b[0;32m    320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_datetime64_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/tslib.pyx\u001b[0m in \u001b[0;36mpandas.tslib.array_to_datetime (pandas/tslib.c:25993)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/tslib.pyx\u001b[0m in \u001b[0;36mpandas.tslib.parse_datetime_string (pandas/tslib.c:24196)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/dateutil/parser.pyc\u001b[0m in \u001b[0;36mparse\u001b[1;34m(timestr, parserinfo, **kwargs)\u001b[0m\n\u001b[0;32m   1006\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparserinfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1007\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1008\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDEFAULTPARSER\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/dateutil/parser.pyc\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, timestr, default, ignoretz, tzinfos, **kwargs)\u001b[0m\n\u001b[0;32m    390\u001b[0m             \u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskipped_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/dateutil/parser.pyc\u001b[0m in \u001b[0;36m_parse\u001b[1;34m(self, timestr, dayfirst, yearfirst, fuzzy, fuzzy_with_tokens)\u001b[0m\n\u001b[0;32m    563\u001b[0m                             \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msecond\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 565\u001b[1;33m                     elif ((i < len_l and info.hms(l[i]) is not None) or\n\u001b[0m\u001b[0;32m    566\u001b[0m                           (i+1 < len_l and l[i] == ' ' and\n\u001b[0;32m    567\u001b[0m                            info.hms(l[i+1]) is not None)):\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/dateutil/parser.pyc\u001b[0m in \u001b[0;36mhms\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    294\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Convert date to actual date format. This might take a while!\n",
    "sfdata.Date = sfdata['Date'].apply(lambda x: pd.to_datetime(x, errors='raise'))\n",
    "sfdata.Time = sfdata['Time'].apply(lambda x: pd.to_datetime(x, errors='raise'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buckets(series, n):\n",
    "    # Takes a series and returns a series mapping each element to a\n",
    "    # one of n buckets.\n",
    "    mi, ma = series.min(), series.max()\n",
    "    buckets = np.linspace(mi, ma, n)\n",
    "    print buckets\n",
    "    def f(e):\n",
    "        for i, el in enumerate(buckets):\n",
    "            if e < el:\n",
    "                return i\n",
    "        return n - 1\n",
    "            \n",
    "    res = series.map(f)\n",
    "    return res\n",
    "\n",
    "def cleanColumns(data):\n",
    "    # Used to rename the columns in our data grame to their appropriate names.\n",
    "    # Also drops unnecessary columns.\n",
    "    data['Latitude'] = data['Y']\n",
    "    data['Longitude'] = data['X']\n",
    "    data['Type'] = data['Category']\n",
    "    \n",
    "    print data.columns\n",
    "    data = data.drop(['IncidntNum', 'Category', 'Descript', 'PdDistrict','Resolution','Address','X','Y', 'Location'], axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def createPartitions(data, n):\n",
    "    # Remove outliers from the latitude/longitude issues\n",
    "    # We know that the city lies between -130, -120 longitude\n",
    "    # We also know that the citiy lies between 37 and 40 degrees latitude\n",
    "    data = data[-120 > data.Longitude][data.Longitude > (-130)]\n",
    "    data = data[data.Latitude > 37][data.Latitude < 40]\n",
    "    \n",
    "    # Each row is an occurrance of a single crime. \n",
    "    # Keep around the original data\n",
    "    data['Region'] =  n *  buckets(data['Latitude'], n) + buckets(data['Longitude'],n) + 1\n",
    "    \n",
    "    # Add in the types into the results.\n",
    "    mapping = {key: i for i,key in enumerate(data['Type'].unique())}\n",
    "    data['TypeIndex'] = data['Type'].map(mapping)\n",
    "\n",
    "    # Now we can add the crime counts. \n",
    "    data['CrimeCount'] = np.ones(len(data))\n",
    "    return data\n",
    "\n",
    "def extractDateFeatures(data):\n",
    "    # Creates a new data frame and returns it as copy with all the data that we're interested in\n",
    "    # Create map from week days to integers\n",
    "    DayOfWeek = {'Sunday': 1,\n",
    "                 'Monday': 2,\n",
    "                 'Tuesday': 3,\n",
    "                 'Wednesday': 4,\n",
    "                 'Thursday': 5,\n",
    "                 'Friday': 6,\n",
    "                 'Saturday': 7 }\n",
    "    data['DoW'] = data['DayOfWeek'].map(DayOfWeek)\n",
    "    data = data.drop('DayOfWeek', axis=1)\n",
    "    print \"Created Weeks\"\n",
    "    \n",
    "    # We assume that the Date column is already in datetime format\n",
    "    data['Month'] = data.Date.map(lambda x: x.month)\n",
    "    data['DoM'] = data.Date.map(lambda x: x.day)\n",
    "    data['Year'] = data.Date.map(lambda x: x.year) - data.Date.min().year\n",
    "    data['ToD'] = data.Time.map(lambda x: x.minute)\n",
    "    data['Time'] = data.Time.map(lambda x: x.value / 10**9) - data.Date.min().value / 10**9\n",
    "    \n",
    "    # We add an additional column that combines the month and the year into number of months since beginning\n",
    "    data['TimeFeature'] = data.ix[:, ['Year', 'Month']].apply(lambda s: 12*s[0] + s[1], axis=1)\n",
    "    \n",
    "    data = data.drop('Date', axis=1)\n",
    "    \n",
    "    print \"Created the time data features!\"\n",
    "    \n",
    "    return data\n",
    "\n",
    "def extractDataFeatures(data, n):\n",
    "    # Given the input data read directly from the exported data \n",
    "    # (https://data.sfgov.org/Public-Safety/Map-Crime-Incidents-from-1-Jan-2003/gxxq-x39z)\n",
    "    # We convert it into the format specified as follows:\n",
    "    # We want the results to be a data frame with the following columns.:\n",
    "    # -> Latitude (float)\n",
    "    # -> Longtitude (float)\n",
    "    # -> Region (specifies which region this data point belongs to)\n",
    "    # -> DoW (1-7 results) (Day of the Week)\n",
    "    # -> Month (1-12) (Month of the Year)\n",
    "    # -> DoM (1-31) (Day of the Month)\n",
    "    # -> Year (0->max(year) - min(year)) \n",
    "    # -> ToD (Time of Day, specified as number of minutes since start of day)\n",
    "    # -> Time (int) : #minutes since earliest sample in the data set\n",
    "    # -> Type (string) : Described the type of crime\n",
    "    # -> TypeIndex (int): Index mapping uniquely each crime type to a value in [1...#crime types]\n",
    "    # -> CrimeCount (int) : The number of crimes in this area\n",
    "    \n",
    "    # Remove unnecessary columns and rename\n",
    "    cData = cleanColumns(data)\n",
    "    \n",
    "    # Created partitions. Note that this modifies the data by adding a REGION column.\n",
    "    partitionedData = createPartitions(cData, n)\n",
    "    \n",
    "    # Now we convert the data to the correct dates and clean the data!\n",
    "    finalData = extractDateFeatures(partitionedData)   \n",
    "    \n",
    "    # Return the results\n",
    "    return finalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countCrime(d, region):\n",
    "        '''\n",
    "        Counts the crime in a given region, returning an array of size 144 with halucinated empty data \n",
    "        for non-existent crime periods.\n",
    "        '''\n",
    "        res = np.zeros(144)\n",
    "        for i in range(144):\n",
    "            try:\n",
    "                # print d.ix[region, i].CrimeCount\n",
    "                res[i] = d.ix[region, i].CrimeCount\n",
    "            except (KeyError, AttributeError, IndexError) as e:\n",
    "                pass\n",
    "    \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = train.groupby(['Region', 'TimeFeature']).aggregate(np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d.ix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def splitTrainTest(D, year=None):\n",
    "    '''\n",
    "    Given a data frame with the specified data, we split into a training set and a test set.\n",
    "    The test data consists of us holding out a particular year from our training data.\n",
    "    '''\n",
    "    # We only want to keep some of the data\n",
    "    D = D.ix[:, ['Year', 'TimeFeature', 'CrimeCount', 'Region']]\n",
    "    \n",
    "    # First, we're going to seperate by region.\n",
    "    if year is None:\n",
    "        test = D[D['Year'] == D['Year'].max()]\n",
    "        train = D[D['Year'] != D['Year'].max()]\n",
    "    else:\n",
    "        test = D[D['Year'] == year]\n",
    "        train = D[D['Year'] != year]\n",
    "        \n",
    "    # Now we keep only a subset of the columns we want, which is TimeFeature and CrimeCount\n",
    "    train = train.drop('Year', axis=1)\n",
    "    test = test.drop('Year', axis=1)\n",
    "\n",
    "    print \"Finished Creating Testing Set\"\n",
    "    \n",
    "    # Now we groupby TimeFeature which consists of YearMonth string.\n",
    "    trainD, testD = train.groupby(['Region', 'TimeFeature']).aggregate(np.sum), test.groupby([ 'Region', 'TimeFeature']).aggregate(np.sum)\n",
    "    # return trainD\n",
    "    trainRes = {}\n",
    "    testRes = {}\n",
    "    for region in range(1,D.Region.max()+1):\n",
    "        # Training\n",
    "        trainRes[region] = np.zeros((144,2))\n",
    "        trainRes[region][:,0] = range(144)\n",
    "        # print trainRes[region]\n",
    "        trainRes[region][:,1] = countCrime(trainD, region)\n",
    "        \n",
    "        # Test \n",
    "        testRes[region] = np.zeros((144,2))\n",
    "        testRes[region][:,0] = range(144) \n",
    "        testRes[region][:,1] = countCrime(testD, region)\n",
    "    \n",
    "    return trainRes, trainRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = extractDataFeatures(sfdata, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotHistogram(results, n):\n",
    "    fig, ax = plt.subplots( nrows=1, ncols=1 )\n",
    "    ax.hist(results.Region, bins=range(n*n))\n",
    "    plt.xlabel(\"San Francisco Region\")\n",
    "    plt.ylabel(\"Total Incidents of Reported Incidents\")\n",
    "    plt.title(\"Crime Incidents in San Francisco when Divided into {}x{} grid.\".format(n,n))\n",
    "    plt.savefig(\"figures/sf_n{}\".format(n))\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plotHistogram(results, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = splitTrainTest(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getRMSE(x,y):\n",
    "    return np.sqrt(np.sum((y - x)**2) / len(y))\n",
    "\n",
    "def trainAndTest(train, test):\n",
    "    # Given a set of training and testing data, train a linear regression model, tests it, and then \n",
    "    # computes the RMSE of each region, returning the resulting dictionary of RMSEs for region, as well as\n",
    "    # a dictionary of predictions, and a dictionary of trained models\n",
    "    models = {}\n",
    "    RMSE = {}\n",
    "    predictions = {}\n",
    "    for region in train:\n",
    "        if region % 10 == 0:\n",
    "            print \"Training on region {}\".format(region)\n",
    "        model = LinearRegression()\n",
    "        x = train[region][:,0]\n",
    "        x = x.reshape((len(x),1))\n",
    "        y = train[region][:,1]\n",
    "        y = y.reshape((len(y),1))\n",
    "        model.fit(x,y)\n",
    "        xTest = test[region][:,0]\n",
    "        xTest = xTest.reshape((len(xTest),1))\n",
    "        yTest = test[region][:,1]\n",
    "        yTest = yTest.reshape((len(yTest),1))\n",
    "        preds = model.predict(xTest)\n",
    "        rmse = getRMSE(yTest, preds)\n",
    "        \n",
    "        # store the results\n",
    "        models[region] = model\n",
    "        RMSE[region] = rmse\n",
    "        predictions[region] = preds\n",
    "    \n",
    "    return models, RMSE, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models, RMSE, predictions = trainAndTest(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sum(RMSE.values()) / len(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tRes = createPartitions(results, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We try different values of n and calculate the average RMSE for each value!\n",
    "# Note that results already has the data extracted\n",
    "rmses = []\n",
    "max_rmses = []\n",
    "min_rmses = []\n",
    "for n in range(1,10) + range(10,50,10):\n",
    "    # We only need to extract the data once, which results has already done (for n = 10)\n",
    "    tRes = createPartitions(results, n)\n",
    "    # This saves a plot of the distribution as a histogram\n",
    "    plotHistogram(tRes, n)\n",
    "    \n",
    "    # Now we split into train and test\n",
    "    train, test = splitTrainTest(tRes)\n",
    "    \n",
    "    # Now we caculate the average RMSE\n",
    "    _, RMSE, _ = trainAndTest(train, test)\n",
    "    print \"RMSE: {}\".format(sum(RMSE.values()) / len(RMSE))\n",
    "    rmses.append(sum(RMSE.values()) / len(RMSE))\n",
    "    max_rmses.append(max(RMSE.values()))\n",
    "    min_rmses.append(min(RMSE.values()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x =range(1,10) + range(10,50,10)\n",
    "plt.scatter(x, rmses, color='red')\n",
    "plt.scatter(x, max_rmses, color='blue')\n",
    "plt.title(\"RMSE vs Resolution\")\n",
    "plt.xlabel(\"Geographic Resolution (n)\")\n",
    "plt.ylabel(\"Average and Max RMSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zip(x,rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
